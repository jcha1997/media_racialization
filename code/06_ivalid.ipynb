{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8530779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Libraries for multiprocessing\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816745f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_random_words(word_vectors, num_words):\n",
    "    # Grab a random sample of words from the word vectors\n",
    "    random_words = random.choices(list(word_vectors.key_to_index.keys()), k=min(num_words, len(word_vectors.key_to_index)))\n",
    "    return random_words\n",
    "\n",
    "def calculate_nearest_neighbors_correlation(word_vectors1, word_vectors2, words, num_neighbors=10):\n",
    "    correlations = []\n",
    "    for word in words:\n",
    "        # Ensure the word exists in both models\n",
    "        if word in word_vectors1.wv.key_to_index and word in word_vectors2.wv.key_to_index:\n",
    "            neighbors1 = [neighbor[0] for neighbor in word_vectors1.wv.most_similar(word, topn=num_neighbors)]\n",
    "            neighbors2 = [neighbor[0] for neighbor in word_vectors2.wv.most_similar(word, topn=num_neighbors)]\n",
    "            common_neighbors = set(neighbors1) & set(neighbors2)\n",
    "            if len(common_neighbors) >= 2:  # Ensure at least 2 common neighbors\n",
    "                ranks1 = [neighbors1.index(neighbor) for neighbor in common_neighbors]\n",
    "                ranks2 = [neighbors2.index(neighbor) for neighbor in common_neighbors]\n",
    "                correlation = np.corrcoef(ranks1, ranks2)[0, 1]\n",
    "                correlations.append(correlation)\n",
    "    if correlations:  # Check if the correlations list is not empty\n",
    "        avg_correlation = np.mean(correlations)\n",
    "    else:\n",
    "        avg_correlation = np.nan  # Set to NaN if correlations list is empty\n",
    "    return avg_correlation\n",
    "\n",
    "\n",
    "def main(corpus, policy):\n",
    "    num_cores = mp.cpu_count()\n",
    "    \n",
    "    # Hyperparameters\n",
    "    window_sizes_1 = [1, 6, 48]  # Example window sizes for model 1\n",
    "    window_sizes_2 = [1, 6, 48]  # Example window sizes for model 2\n",
    "    dimension_sizes_1 = [50, 300, 450]  # Example dimension sizes for model 1\n",
    "    dimension_sizes_2 = [50, 300, 450]  # Example dimension sizes for model 2\n",
    "    num_words = 100  # Number of random words to sample\n",
    "\n",
    "    # Data storage\n",
    "    data = []\n",
    "\n",
    "    for window_size_1 in window_sizes_1:\n",
    "        for dimension_size_1 in dimension_sizes_1:\n",
    "            for window_size_2 in window_sizes_2:\n",
    "                for dimension_size_2 in dimension_sizes_2:\n",
    "                    # Train Word2Vec models\n",
    "                    model1 = Word2Vec(corpus, window=window_size_1, vector_size=dimension_size_1, min_count = 100, sg = 1, hs = 0, negative = 5, workers = num_cores-1)\n",
    "                    model2 = Word2Vec(corpus, window=window_size_2, vector_size=dimension_size_2,  min_count = 100, sg = 1, hs = 0, negative = 5, workers = num_cores-1)\n",
    "\n",
    "                    # Grab random words\n",
    "                    words = grab_random_words(model1.wv, num_words)\n",
    "\n",
    "                    # Calculate correlation of nearest neighbors\n",
    "                    correlations = calculate_nearest_neighbors_correlation(model1, model2, words)\n",
    "\n",
    "                    # Store data\n",
    "                    data.append({'Window Size 1': window_size_1,\n",
    "                                 'Dimension Size 1': dimension_size_1,\n",
    "                                 'Window Size 2': window_size_2,\n",
    "                                 'Dimension Size 2': dimension_size_2,\n",
    "                                 'Average Correlation': correlations})\n",
    "                    \n",
    "                    print(window_size_1, dimension_size_1, window_size_2, dimension_size_2, correlations)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Window Size 1', 'Dimension Size 1', 'Window Size 2', 'Dimension Size 2', 'Average Correlation'])\n",
    "\n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(policy + \"ivalid.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c46bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 50 1 50 0.6419017741753815\n"
     ]
    }
   ],
   "source": [
    "with open('intermediate/healthcare_corpus.pickle', 'rb') as f:\n",
    "    list_of_lists = pickle.load(f)\n",
    "    \n",
    "corpus = []\n",
    "flatten = [item for sublist in list_of_lists for item in sublist]\n",
    "corpus.extend(flatten)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(corpus, \"healthcare\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
